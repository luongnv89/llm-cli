#!/bin/bash
# llm-cli: Local LLM management tool
# Main entry point and command dispatcher

set -euo pipefail

# Get script directory (resolves symlinks)
get_script_dir() {
    local source="${BASH_SOURCE[0]}"
    local dir
    while [ -L "$source" ]; do
        dir="$(cd -P "$(dirname "$source")" && pwd)"
        source="$(readlink "$source")"
        [[ $source != /* ]] && source="$dir/$source"
    done
    cd -P "$(dirname "$source")" && pwd
}

SCRIPT_DIR="$(get_script_dir)"
LIB_DIR="$(dirname "$SCRIPT_DIR")/lib"

# Source library files
source "$LIB_DIR/config.sh"
source "$LIB_DIR/utils.sh"
source "$LIB_DIR/models.sh"
source "$LIB_DIR/download.sh"
source "$LIB_DIR/chat.sh"
source "$LIB_DIR/benchmark.sh"
source "$LIB_DIR/stats.sh"
source "$LIB_DIR/dev-info.sh"

# Show main help
show_help() {
    cat <<EOF
${BOLD}llm-cli${RESET} - Local LLM management tool v${LLM_CLI_VERSION}

${BOLD}USAGE:${RESET}
    llm-cli <command> [options] [arguments]

${BOLD}COMMANDS:${RESET}
    ${CYAN}search${RESET}, s <query>       Search HuggingFace for GGUF models
    ${CYAN}download${RESET}, d <repo>      Download a model from HuggingFace
    ${CYAN}chat${RESET}, c, run [N]        Start conversation with a model
    ${CYAN}models${RESET}                  Model management commands
    ${CYAN}bench${RESET}, benchmark        Benchmark models
    ${CYAN}stats${RESET}                   Show usage statistics
    ${CYAN}config${RESET}                  Show/edit configuration
    ${CYAN}info${RESET}                    Show developer integration information

${BOLD}MODEL MANAGEMENT:${RESET}
    llm-cli models list         List all cached models
    llm-cli models info <N>     Show detailed model info
    llm-cli models delete <N>   Delete a cached model
    llm-cli models update <N>   Update a cached model

${BOLD}BENCHMARKING:${RESET}
    llm-cli bench [N]           Benchmark a single model
    llm-cli bench --all         Benchmark all cached models
    llm-cli bench --batch 1,2,3 Benchmark specific models

${BOLD}OPTIONS:${RESET}
    -h, --help                  Show this help message
    -v, --version               Show version
    --platform <name>           Override platform detection (macos, linux-nvidia, linux-cpu)
    --no-color                  Disable colored output

${BOLD}EXAMPLES:${RESET}
    llm-cli search llama-3.2
    llm-cli download bartowski/Llama-3.2-3B-Instruct-GGUF
    llm-cli chat
    llm-cli chat 1
    llm-cli models list
    llm-cli bench --all
    llm-cli stats
    llm-cli info
    llm-cli info --json

${BOLD}CONFIGURATION:${RESET}
    Config file: ~/.config/llm-cli/config
    Data dir:    ~/.local/share/llm-cli/

${BOLD}ENVIRONMENT VARIABLES:${RESET}
    LLM_CLI_THREADS       Override thread count
    LLM_CLI_GPU_LAYERS    Override GPU layers
    LLM_CLI_CONTEXT_SIZE  Override context size
    LLM_CLI_PLATFORM      Override platform detection
    NO_COLOR              Disable colored output

${BOLD}SUPPORTED PLATFORMS:${RESET}
    macos                 macOS with Apple Silicon (Metal GPU)
    linux-nvidia          Linux with NVIDIA GPU (CUDA)
    linux-cpu             Linux CPU-only mode

EOF
}

# Show version
show_version() {
    echo "llm-cli version $LLM_CLI_VERSION"
}

# Handle models subcommand
cmd_models() {
    local subcmd="${1:-list}"
    shift || true

    case "$subcmd" in
        list | ls)
            cmd_models_list "$@"
            ;;
        info | i)
            cmd_models_info "$@"
            ;;
        delete | rm)
            cmd_models_delete "$@"
            ;;
        update | up)
            cmd_models_update "$@"
            ;;
        -h | --help)
            echo "Usage: llm-cli models <subcommand> [args]"
            echo ""
            echo "Subcommands:"
            echo "  list, ls        List all cached models"
            echo "  info <N>        Show detailed model info"
            echo "  delete, rm <N>  Delete a cached model"
            echo "  update, up <N>  Update a cached model"
            ;;
        *)
            log_error "Unknown models subcommand: $subcmd"
            echo "Run 'llm-cli models --help' for usage"
            exit 1
            ;;
    esac
}

# Handle config subcommand
cmd_config() {
    local subcmd="${1:-show}"
    shift || true

    case "$subcmd" in
        show | "")
            show_config
            ;;
        --edit | -e | edit)
            edit_config
            ;;
        -h | --help)
            echo "Usage: llm-cli config [--edit]"
            echo ""
            echo "Options:"
            echo "  (none)    Show current configuration"
            echo "  --edit    Open config file in editor"
            ;;
        *)
            show_config
            ;;
    esac
}

# Handle info command
cmd_info() {
    local endpoint_override=""
    local port_override=""
    local output_format="text" # text, json, endpoint-only, port-only, status-only
    local template_format=""   # openai, anthropic, env, examples

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case "$1" in
            -h | --help)
                render_info_help
                return 0
                ;;
            --endpoint)
                endpoint_override="$2"
                shift 2
                ;;
            --endpoint=*)
                endpoint_override="${1#--endpoint=}"
                shift
                ;;
            --port)
                port_override="$2"
                shift 2
                ;;
            --port=*)
                port_override="${1#--port=}"
                shift
                ;;
            --json)
                output_format="json"
                shift
                ;;
            --endpoint-only)
                output_format="endpoint-only"
                shift
                ;;
            --port-only)
                output_format="port-only"
                shift
                ;;
            --status-only)
                output_format="status-only"
                shift
                ;;
            --format)
                template_format="$2"
                output_format="template"
                shift 2
                ;;
            --format=*)
                template_format="${1#--format=}"
                output_format="template"
                shift
                ;;
            -*)
                log_error "Unknown option: $1"
                return 1
                ;;
            *)
                log_error "Unknown argument: $1"
                return 1
                ;;
        esac
    done

    # Get endpoint URL
    local endpoint
    endpoint=$(get_endpoint_url "$endpoint_override")

    # Extract port
    local port
    if [[ -n "$port_override" ]]; then
        port="$port_override"
    else
        port=$(get_port_from_endpoint "$endpoint")
    fi

    # Check endpoint status
    local status
    status=$(check_endpoint_status "$endpoint" 2) || status="not-running"

    # Render output based on format
    case "$output_format" in
        text)
            render_info_text "$endpoint" "$port" "$status"
            ;;
        json)
            render_info_json "$endpoint" "$port" "$status"
            ;;
        endpoint-only)
            echo "$endpoint"
            ;;
        port-only)
            echo "$port"
            ;;
        status-only)
            echo "$status"
            ;;
        template)
            case "$template_format" in
                openai)
                    render_openai_config "$endpoint"
                    ;;
                anthropic)
                    render_anthropic_config "$endpoint"
                    ;;
                env)
                    render_env_vars "$endpoint" "$port"
                    ;;
                examples)
                    render_code_examples "$endpoint"
                    ;;
                *)
                    log_error "Unknown format: $template_format"
                    echo "Available formats: openai, anthropic, env, examples"
                    return 1
                    ;;
            esac
            ;;
    esac
}

# Main dispatcher
main() {
    local platform_override=""

    # Pre-parse --platform flag before loading config
    # This allows platform to affect default configuration
    for arg in "$@"; do
        if [[ "$arg" == "--platform" ]]; then
            # Next arg should be the platform value
            local next=false
            for a in "$@"; do
                if [[ "$next" == "true" ]]; then
                    platform_override="$a"
                    break
                fi
                [[ "$a" == "--platform" ]] && next=true
            done
            break
        elif [[ "$arg" == --platform=* ]]; then
            platform_override="${arg#--platform=}"
            break
        fi
    done

    # Set platform (affects defaults)
    set_platform "$platform_override"

    # Initialize configuration with platform-specific defaults
    init_config
    load_config
    setup_colors

    # Parse global options
    while [[ $# -gt 0 ]]; do
        case "$1" in
            -h | --help)
                show_help
                exit 0
                ;;
            -v | --version)
                show_version
                exit 0
                ;;
            --platform)
                # Already handled above, just skip
                shift
                shift
                ;;
            --platform=*)
                # Already handled above, just skip
                shift
                ;;
            --no-color)
                NO_COLOR=1
                setup_colors
                shift
                ;;
            -*)
                log_error "Unknown option: $1"
                echo "Run 'llm-cli --help' for usage"
                exit 1
                ;;
            *)
                break
                ;;
        esac
    done

    # Get command
    local command="${1:-}"
    shift || true

    # Dispatch to command handler
    case "$command" in
        "")
            show_help
            ;;
        search | s)
            cmd_search "$@"
            ;;
        download | d | get)
            cmd_download "$@"
            ;;
        chat | c | run)
            cmd_chat "$@"
            ;;
        models | model)
            cmd_models "$@"
            ;;
        bench | benchmark)
            cmd_bench "$@"
            ;;
        stats | statistics)
            cmd_stats "$@"
            ;;
        config)
            cmd_config "$@"
            ;;
        info)
            cmd_info "$@"
            ;;
        help)
            show_help
            ;;
        *)
            log_error "Unknown command: $command"
            echo "Run 'llm-cli --help' for usage"
            exit 1
            ;;
    esac
}

# Run main
main "$@"
