# llm-cli

A modular command-line tool for managing and running local LLMs with llama.cpp. Supports macOS (Apple Silicon), Linux with NVIDIA GPUs (including DGX Spark), and Linux CPU-only systems.

## Features

- **Search & Download**: Search HuggingFace for GGUF models and download with auto-quantization selection
- **Model Management**: List, inspect, delete, and update cached models
- **Chat**: Start interactive conversations with any cached model
- **Benchmarking**: Benchmark single models, batches, or all cached models
- **Statistics**: Track usage with full session history
- **Cross-Platform**: Auto-detects platform and applies optimal settings for your hardware

## Supported Platforms

| Platform | GPU Acceleration | Default Threads |
|----------|-----------------|-----------------|
| macOS (Apple Silicon) | Metal | 8 |
| Linux + NVIDIA GPU | CUDA | 10 (DGX Spark P-cores) |
| Linux (CPU-only) | None | Auto (all cores) |

## Installation

### macOS

```bash
# Install llama.cpp
brew install llama.cpp

# Clone and install llm-cli
git clone https://github.com/luongnv89/llm-cli.git
cd llm-cli
./install.sh
```

### Ubuntu / Linux with NVIDIA GPU (DGX Spark)

```bash
# Install build dependencies
sudo apt update && sudo apt install -y build-essential cmake git

# Build llama.cpp with CUDA support
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
sudo cp build/bin/llama-* /usr/local/bin/

# Install llm-cli
cd ..
git clone https://github.com/luongnv89/llm-cli.git
cd llm-cli
./install.sh
```

### Ubuntu / Linux (CPU-only)

```bash
# Install build dependencies
sudo apt update && sudo apt install -y build-essential cmake git

# Build llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
cmake -B build
cmake --build build --config Release
sudo cp build/bin/llama-* /usr/local/bin/

# Install llm-cli
cd ..
git clone https://github.com/luongnv89/llm-cli.git
cd llm-cli
./install.sh
```

The installer creates a symlink in `~/.local/bin/`. Make sure this is in your PATH.

## Quick Start

```bash
# Quick way - auto-download & chat (no pre-download needed)
llm-cli chat bartowski/Llama-3.2-3B-Instruct-GGUF

# Or the traditional way - download first, then chat
llm-cli search llama-3.2
llm-cli download bartowski/Llama-3.2-3B-Instruct-GGUF
llm-cli models list
llm-cli chat

# Run a benchmark
llm-cli bench
```

## Commands

### Search & Download

```bash
# Search HuggingFace for GGUF models
llm-cli search <query>
llm-cli s llama-3.2

# Download a specific repository
llm-cli download <repo>
llm-cli d bartowski/Llama-3.2-3B-Instruct-GGUF
```

### Chat

```bash
# Auto-download & chat with HuggingFace model (recommended)
llm-cli chat bartowski/Llama-3.2-3B-Instruct-GGUF

# Auto-download with specific quantization
llm-cli chat bartowski/Llama-3.2-3B-Instruct-GGUF/Q4_K_M.gguf

# Send prompt and exit (non-interactive)
llm-cli chat bartowski/Llama-3.2-3B-Instruct-GGUF "What is 2+2?"

# Interactive model selection (from cached models)
llm-cli chat

# Run specific cached model by number
llm-cli chat 1
llm-cli c 2
```

### Model Management

```bash
# List all cached models
llm-cli models list

# Show detailed model info
llm-cli models info 1

# Delete a cached model
llm-cli models delete 1

# Update/re-download a model
llm-cli models update 1
```

### Benchmarking

```bash
# Benchmark single model (interactive)
llm-cli bench

# Benchmark specific model
llm-cli bench 1

# Benchmark all cached models
llm-cli bench --all

# Benchmark specific models
llm-cli bench --batch 1,2,3

# Save reports to custom directory
llm-cli bench --output ./my-reports
llm-cli bench --all -o /tmp/benchmarks

# View saved benchmark reports
llm-cli bench --reports
```

Benchmark results are automatically saved as Markdown reports. By default, reports are saved to `~/.local/share/llm-cli/benchmarks/`, but you can specify a custom directory with `--output`. Reports include:
- Model information (name, path, size)
- System information (platform, CPU/GPU, memory, llama.cpp version)
- NVIDIA GPU details (model, memory, temperature, power) when applicable
- Benchmark configuration (threads, GPU layers, tokens)
- Performance results (prompt processing, text generation speeds)
- Raw benchmark output

### Statistics

```bash
# Show usage statistics
llm-cli stats

# Clear all statistics
llm-cli stats --clear
```

### Configuration

```bash
# Show current configuration (includes detected platform)
llm-cli config

# Edit configuration file
llm-cli config --edit
```

## Configuration

Configuration is stored at `~/.config/llm-cli/config`. Default values are set based on your detected platform:

```bash
# Performance settings (auto-configured for your platform)
THREADS=8          # 8 for macOS, 10 for NVIDIA, auto for CPU-only
GPU_LAYERS=99      # 99 for GPU platforms, 0 for CPU-only
CONTEXT_SIZE=4096

# System prompt for chat sessions
SYSTEM_PROMPT="You are a helpful AI assistant."

# Output settings
NO_COLOR=0
VERBOSE=1
```

### Environment Variables

Override settings with environment variables:

```bash
LLM_CLI_THREADS=4 llm-cli chat
LLM_CLI_GPU_LAYERS=32 llm-cli bench
LLM_CLI_PLATFORM=linux-cpu llm-cli config  # Force platform
```

### Platform Override

Override auto-detected platform via command-line flag or environment variable:

```bash
# Command-line flag (highest priority)
llm-cli --platform linux-nvidia bench
llm-cli --platform linux-cpu chat
llm-cli --platform macos config

# Environment variable
LLM_CLI_PLATFORM=linux-nvidia llm-cli bench
```

Valid platform values: `macos`, `linux-nvidia`, `linux-cpu`

## Quantization Optimization

### MXFP4 for NVIDIA Blackwell (DGX Spark)

When searching for models on **DGX Spark** systems, llm-cli automatically prioritizes **MXFP4** quantization:

```
llm-cli search llama-3.2
# Shows: "ðŸ’¡ Platform Optimization Tip:
#         MXFP4 is specifically optimized for Blackwell architecture (DGX Spark)
#         Prefer MXFP4 models over standard Q4_K_M for best performance"
```

**Why MXFP4?**
- Specifically optimized for NVIDIA Blackwell architecture used in DGX Spark
- Delivers superior performance compared to standard quantizations
- Maintains accuracy while reducing memory requirements
- Ideal for running larger models on GPU

**How it works:**
1. When searching for models, look for MXFP4 variants first
2. During auto-download, MXFP4 is automatically selected if available
3. Fall back to Q5_K_M, Q4_K_M if MXFP4 not available

### Standard Quantization Priority

For other platforms (macOS, Linux CPU), the default quantization priority is:
1. Q5_K_M - Balanced quality and size
2. Q4_K_M - More aggressive quantization
3. Q6_K - Higher quality for larger contexts
4. Q4_K_S - Small quantization
5. Q8_0 - Full 8-bit quantization

## Data Storage

Following XDG Base Directory specification:

- **Config**: `~/.config/llm-cli/config`
- **Data**: `~/.local/share/llm-cli/stats.json`
- **Benchmarks**: `~/.local/share/llm-cli/benchmarks/` (Markdown reports)
- **Cache**: `~/.cache/llm-cli/`
- **Models**: `~/.cache/huggingface/hub/` (standard HuggingFace cache)

## Dependencies

### macOS

**Required:**
- `llama.cpp` (`brew install llama.cpp`)

**Optional:**
- `jq` - For full statistics features (`brew install jq`)

### Linux

**Required:**
- `llama.cpp` (build from source, see Installation)
- For NVIDIA GPU: CUDA toolkit and drivers

**Optional:**
- `jq` - For full statistics features (`sudo apt install jq`)

## Project Structure

```
llm-cli/
â”œâ”€â”€ bin/
â”‚   â””â”€â”€ llm-cli          # Main entry point
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ config.sh        # Configuration and platform detection
â”‚   â”œâ”€â”€ utils.sh         # Utilities and logging
â”‚   â”œâ”€â”€ models.sh        # Model management
â”‚   â”œâ”€â”€ download.sh      # Search and download
â”‚   â”œâ”€â”€ chat.sh          # Chat/conversation
â”‚   â”œâ”€â”€ benchmark.sh     # Benchmarking
â”‚   â””â”€â”€ stats.sh         # Statistics tracking
â”œâ”€â”€ completions/
â”‚   â”œâ”€â”€ llm-cli.bash     # Bash completion
â”‚   â””â”€â”€ llm-cli.zsh      # Zsh completion
â”œâ”€â”€ install.sh           # Cross-platform installation script
â””â”€â”€ README.md
```

## Development

### Prerequisites

For development, install these additional tools:

**macOS:**
```bash
brew install shfmt shellcheck
pip install pre-commit
```

**Ubuntu:**
```bash
sudo apt install shfmt shellcheck
pip install pre-commit
```

### Setup

```bash
# Clone the repository
git clone https://github.com/luongnv89/llm-cli.git
cd llm-cli

# Install pre-commit hooks
pre-commit install
```

### Running Quality Checks

```bash
# Run all checks (same as CI)
pre-commit run --all-files

# Run individual checks
shfmt -i 4 -bn -ci -d bin/llm-cli lib/*.sh
shellcheck --severity=warning bin/llm-cli lib/*.sh
```

### Code Style

- 4-space indentation
- Use `set -euo pipefail` for strict error handling
- Quote all variable expansions
- Follow existing patterns in `lib/` modules

## License

MIT
